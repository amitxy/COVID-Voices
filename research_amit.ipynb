{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0234d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yandex/MLWG2025/amitr5/project/anaconda3/envs/covid/lib/python3.13/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "import numpy as np\n",
    "import torch\n",
    "import wandb\n",
    "import os\n",
    "from covid_voices.data.datasets.corona_dataset import CoronaTweetDataset\n",
    "from datasets import Dataset, DatasetDict\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d274e0",
   "metadata": {},
   "source": [
    "## Some Examples of usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84c22ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded datasets:\n",
      "dict_keys(['train', 'val', 'test'])\n",
      "\n",
      "See the preprocess_tweet function in action :\n",
      "\"consumer voice has compiled a list of creative ideas and best practices for staying connected during the pandemic.  the list includes ways to communicate with loved ones and ideas for staying active and engaged while in isolation.\"\n",
      "\n",
      "https://t.co/j9udncqlnn https://t.co/kwrrsjhfkq\n"
     ]
    }
   ],
   "source": [
    "def preprocess_tweet(text):\n",
    "        \"\"\"Clean and normalize tweet text\"\"\"\n",
    "        # Example preprocessing - you can expand this\n",
    "        text = text.lower()\n",
    "        text = text.replace('#', 'hashtag_')\n",
    "        text = text.replace('@', 'mention_')\n",
    "        return text\n",
    "\n",
    "# Load datasets with preprocessing and split into train, validation, test\n",
    "datasets = CoronaTweetDataset.load_datasets(preprocessing=preprocess_tweet, \n",
    "                                            is_val_split=True,\n",
    "                                            val_size=0.2,\n",
    "                                            seed=SEED)\n",
    "\n",
    "print(f\"Loaded datasets:\\n{datasets.keys()}\\n\")\n",
    "print(f'See the preprocess_tweet function in action :\\n{datasets[\"train\"][0][\"text\"]}')\n",
    "\n",
    "\n",
    "# split in train, validation, test\n",
    "train_dataset = datasets[\"train\"]\n",
    "val_dataset = datasets[\"val\"]\n",
    "test_dataset = datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e124fc68",
   "metadata": {},
   "source": [
    "## Using HF libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1b5551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f52438c043f540c8ada782475c68b867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenize_function' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m batch_size = \u001b[32m128\u001b[39m\n\u001b[32m     12\u001b[39m tokenizer = AutoTokenizer.from_pretrained(model_name)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m tokenized_datasets = hf_datasets.map(\u001b[43mtokenize_function\u001b[49m, batched=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     14\u001b[39m tokenized_datasets.set_format(\u001b[38;5;28mtype\u001b[39m=\u001b[33m\"\u001b[39m\u001b[33mtorch\u001b[39m\u001b[33m\"\u001b[39m, columns=[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     16\u001b[39m model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=train_dataset.num_labels)\n",
      "\u001b[31mNameError\u001b[39m: name 'tokenize_function' is not defined"
     ]
    }
   ],
   "source": [
    "hf_datasets = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(train_dataset.df, preserve_index=False),\n",
    "    \"val\": Dataset.from_pandas(val_dataset.df, preserve_index=False),\n",
    "    \"test\": Dataset.from_pandas(test_dataset.df, preserve_index=False)\n",
    "})\n",
    "\n",
    "model_name = \"huawei-noah/TinyBERT_General_4L_312D\"\n",
    "output_base_dir = \"./models/\"\n",
    "project_name = \"corona-NLP-ensemble\"\n",
    "batch_size = 128\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenized_datasets = hf_datasets.map(lambda x: tokenizer(x[\"text\"], padding=\"max_length\", truncation=True), batched=True)\n",
    "tokenized_datasets.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=train_dataset.num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ded8a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abf7b0f6af7c4e6781d57e7db724e80a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/409 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric_names = [\"accuracy\", \"f1\", \"precision\", \"recall\"]\n",
    "metrics = [evaluate.load(name) for name in metric_names]\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    results = {}\n",
    "    for metric in metrics:\n",
    "        # Some metrics (like f1) require average=\"macro\" for multiclass\n",
    "        if metric.name in [\"f1\", \"precision\", \"recall\"]:\n",
    "            res = metric.compute(predictions=predictions, references=labels, average=\"macro\")\n",
    "        else:\n",
    "            res = metric.compute(predictions=predictions, references=labels)\n",
    "        results.update(res)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab90a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = (predictions == labels).astype(float).mean().item()\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "\n",
    "def preprocess(example, tokenizer):\n",
    "    return tokenizer(\n",
    "        example[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=train_dataset.max_length,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5be5ca23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'OriginalTweet': 'Corona virus safety tips #6. Take all the necessary preventive measures. Stay safe, stay healthy!!!\\r\\r\\n#staysafe\\r\\r\\n#healthtips\\r\\r\\n#coronavirus\\r\\r\\n#Shopping #eatery #lounge\\r\\r\\n#acesupermarket\\r\\r\\n#aceeatery\\r\\r\\n#acelounge\\r\\r\\n#acefamily\\r\\r\\n#Ibadan #Oyo #Ogbomoso #Ilorin\\r\\r\\n#Osogbo #Ileife #Ijebuode #Abeokuta https://t.co/LfiLC2RuB5',\n",
       " 'label': 4}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = split[\"train\"]\n",
    "d[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4ff314",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict_values' object has no attribute 'map'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m tokenizer = AutoTokenizer.from_pretrained(model_name)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m tokenized = \u001b[43msplit\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m(\u001b[38;5;28;01mlambda\u001b[39;00m x: preprocess(x, tokenizer, train_dataset.preprocessing), batched=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      3\u001b[39m tokenized.set_format(\u001b[38;5;28mtype\u001b[39m=\u001b[33m\"\u001b[39m\u001b[33mtorch\u001b[39m\u001b[33m\"\u001b[39m, columns=[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m      5\u001b[39m model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=train_dataset.num_labels)\n",
      "\u001b[31mAttributeError\u001b[39m: 'dict_values' object has no attribute 'map'"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenized = split.values().map(lambda x: preprocess(x, tokenizer, train_dataset.preprocessing), batched=True)\n",
    "\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./test_output\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=128,\n",
    "    num_train_epochs=5,\n",
    "    do_train=True,\n",
    "    do_eval=True\n",
    "    )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"val\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaeb9693",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wandb.init(project=project_name, name=model_name, reinit=True)\n",
    "trainer.train()\n",
    "trainer.save_model(os.path.join(output_base_dir, model_name))\n",
    "tokenizer.save_pretrained(os.path.join(output_base_dir, model_name))\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb21a26b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a43cf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"OriginalTweet\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_length)\n",
    "\n",
    "tokenized = raw_datasets.map(tokenize, batched=True)\n",
    "tokenized.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\",\n",
    "\"Sentiment\"])\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=5, id2label=label2id, label2id=label2id)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = (predictions == labels).astype(float).mean().item()\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "args = TrainingArguments(\n",
    "output_dir=\"./results\",\n",
    "eval_strategy=\"epoch\",\n",
    "per_device_train_batch_size=8,\n",
    "per_device_eval_batch_size=8,\n",
    "num_train_epochs=1,\n",
    "logging_dir=\"./logs\",\n",
    "logging_steps=10,\n",
    "save_strategy=\"no\"\n",
    ")\n",
    "trainer = Trainer(\n",
    "model=model,\n",
    "args=args,\n",
    "train_dataset=tokenized[\"train\"],\n",
    "eval_dataset=tokenized[\"test\"],\n",
    "compute_metrics=compute_metrics\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
