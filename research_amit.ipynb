{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c0234d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import wandb\n",
    "\n",
    "from covid_voices.data.datasets.corona_dataset import CoronaTweetDataset\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c22ebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': <covid_voices.data.datasets.corona_dataset.CoronaTweetDataset at 0x706df5561f20>,\n",
       " 'test': <covid_voices.data.datasets.corona_dataset.CoronaTweetDataset at 0x706df49559d0>}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_tweet(text):\n",
    "        \"\"\"Clean and normalize tweet text\"\"\"\n",
    "        # Example preprocessing - you can expand this\n",
    "        text = text.lower()\n",
    "        text = text.replace('#', 'hashtag_')\n",
    "        text = text.replace('@', 'mention_')\n",
    "        return text\n",
    "    \n",
    "datasets = CoronaTweetDataset.load_datasets(preprocessing=preprocess_tweet)\n",
    "datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3069d31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'mention_menyrbie mention_phil_gahan mention_chrisitv https://t.co/ifz9fan2pa and https://t.co/xx6ghgfzcc and https://t.co/i2nlzdxno8',\n",
       " 'label': tensor(2)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaeb9693",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configuration\n",
    "model_names = [\n",
    "     \"distilbert-base-uncased\",\n",
    "    \"huawei-noah/TinyBERT_General_4L_312D\",\n",
    "]\n",
    "num_labels = 5\n",
    "max_length = 128\n",
    "batch_size = 128\n",
    "output_base_dir = \"./models/\"\n",
    "project_name = \"corona-nlp-ensemble\"\n",
    "\n",
    "# Label mapping\n",
    "label2id = {\n",
    "    \"Extremely Negative\": 0,\n",
    "    \"Negative\": 1,\n",
    "    \"Neutral\": 2,\n",
    "    \"Positive\": 3,\n",
    "    \"Extremely Positive\": 4\n",
    "}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "# Load the dataset from local CSV\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_files={\n",
    "    \"train\": \"data/Corona_NLP_train.csv\",\n",
    "    \"test\": \"data/Corona_NLP_test.csv\"\n",
    "},\n",
    "                       encoding=\"latin1\"\n",
    ")\n",
    "\n",
    "# Add encoded label column\n",
    "def encode_labels(example):\n",
    "    example[\"label\"] = label2id[example[\"Sentiment\"]]\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(encode_labels)\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess(example, tokenizer):\n",
    "    return tokenizer(\n",
    "        example[\"OriginalTweet\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_length\n",
    "    )\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = (predictions == labels).astype(float).mean().item()\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "\n",
    "split = dataset[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# Train each model separately\n",
    "for i, model_name in enumerate(model_names):\n",
    "    print(f\"Training model {i+1}/{len(model_names)}: {model_name}\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenized = split.map(lambda x: preprocess(x, tokenizer), batched=True)\n",
    "    tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "    wandb.init(project=project_name, name=f\"model_{i}_{model_name.replace('/', '_')}\", reinit=True)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=num_labels,\n",
    "        id2label=id2label,\n",
    "        label2id=label2id\n",
    "    )\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./test_output\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        per_device_train_batch_size=128,\n",
    "        num_train_epochs=5,\n",
    "        do_train=True,\n",
    "        do_eval=True\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized[\"train\"],\n",
    "        eval_dataset=tokenized[\"test\"],\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model(os.path.join(output_base_dir, f\"model_{i}\"))\n",
    "    tokenizer.save_pretrained(os.path.join(output_base_dir, f\"model_{i}\"))\n",
    "    wandb.finish()\n",
    "\n",
    "print(\"All ensemble models trained and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb21a26b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a43cf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer,AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "raw_datasets = load_dataset(\"csv\",data_files={\"train\": \"data/Corona_NLP_train.csv\", \"test\": \"data/Corona_NLP_test.csv\"}, encoding=\"latin1\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "def tokenize(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"OriginalTweet\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_length)\n",
    "\n",
    "tokenized = raw_datasets.map(tokenize, batched=True)\n",
    "tokenized.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\",\n",
    "\"Sentiment\"])\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=5, id2label=label2id, label2id=label2id)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = (predictions == labels).astype(float).mean().item()\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "args = TrainingArguments(\n",
    "output_dir=\"./results\",\n",
    "eval_strategy=\"epoch\",\n",
    "per_device_train_batch_size=8,\n",
    "per_device_eval_batch_size=8,\n",
    "num_train_epochs=1,\n",
    "logging_dir=\"./logs\",\n",
    "logging_steps=10,\n",
    "save_strategy=\"no\"\n",
    ")\n",
    "trainer = Trainer(\n",
    "model=model,\n",
    "args=args,\n",
    "train_dataset=tokenized[\"train\"],\n",
    "eval_dataset=tokenized[\"test\"],\n",
    "compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5df270",
   "metadata": {},
   "source": [
    "# Best Practices for Handling Train and Test Datasets\n",
    "\n",
    "There are three main approaches to handling train and test datasets in NLP projects:\n",
    "\n",
    "1. **Creating separate dataset objects** - Using separate instances for train and test\n",
    "2. **Using a dataset class with split parameters** - Single class that loads different data based on 'split' parameter\n",
    "3. **Using dataset splitting methods** - Loading all data and then splitting programmatically\n",
    "\n",
    "Our improved `CoronaTweetDataset` class supports all three approaches for maximum flexibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbe0703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of using the recommended approach in a complete training pipeline\n",
    "\n",
    "try:\n",
    "    from datasets.corona_dataset import CoronaTweetDataset\n",
    "    import torch\n",
    "    from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "    \n",
    "    # 1. Load the tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    \n",
    "    # 2. Define a preprocessing function (optional)\n",
    "    def preprocess_tweet(text):\n",
    "        \"\"\"Clean and normalize tweet text\"\"\"\n",
    "        # Example preprocessing - you can expand this\n",
    "        text = text.lower()\n",
    "        text = text.replace('#', 'hashtag_')\n",
    "        text = text.replace('@', 'mention_')\n",
    "        return text\n",
    "    \n",
    "    # 3. Use factory method to load both datasets with consistent parameters\n",
    "   \n",
    "    \n",
    "    # 4. Access individual datasets\n",
    "    train_dataset = datasets[\"train\"]\n",
    "    test_dataset = datasets[\"test\"]\n",
    "    \n",
    "    # 5. Create data loaders\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        num_workers=4\n",
    "    )\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=64,\n",
    "        shuffle=False,\n",
    "        num_workers=4\n",
    "    )\n",
    "    \n",
    "    # 6. Display dataset information\n",
    "    print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "    print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "    \n",
    "    # 7. Show a batch example\n",
    "    print(\"\\nExample batch from training data:\")\n",
    "    for batch in train_loader:\n",
    "        for key, value in batch.items():\n",
    "            if isinstance(value, torch.Tensor):\n",
    "                print(f\"{key} shape: {value.shape}\")\n",
    "            else:\n",
    "                print(f\"{key}: {value}\")\n",
    "        break  # Just show the first batch\n",
    "        \n",
    "except (ImportError, FileNotFoundError) as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Make sure the datasets module is accessible and data files exist.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "covid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
