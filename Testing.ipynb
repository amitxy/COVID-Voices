{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "6k-Q9AxunVpS",
        "OOc1MlLqnhBJ"
      ],
      "machine_shape": "hm",
      "gpuType": "L4",
      "mount_file_id": "1ZmfrWndl3uJqEA3Xyk8zDRg5ujHDDgDo",
      "authorship_tag": "ABX9TyN0OepYHw+msi23hh/9kLqi"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "_zPgfJjEmHkI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FzJDLj8HmABc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from safetensors.torch import load_file\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import kagglehub\n",
        "from kagglehub import KaggleDatasetAdapter\n",
        "import matplotlib.pyplot as plt, seaborn as sns\n",
        "import matplotlib.ticker as mtick\n",
        "from wordcloud import WordCloud\n",
        "import nltk, re, string, warnings, textwrap, datetime as dt\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import re\n",
        "from collections import Counter\n",
        "!pip install -q optuna\n",
        "import optuna, wandb\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from huggingface_hub import upload_file, login, notebook_login, HfApi\n",
        "from datasets import Dataset, Features, Sequence, Value\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoConfig, AutoModelForSequenceClassification,\n",
        "    TrainingArguments, Trainer, EarlyStoppingCallback\n",
        ")\n",
        "from transformers.trainer_callback import TrainerCallback\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPU connection"
      ],
      "metadata": {
        "id": "rd26Bs4OnSne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "id": "pZ9wG3SLnTwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Full-Code Fine-Tuned DeBERTa-v3-base"
      ],
      "metadata": {
        "id": "6k-Q9AxunVpS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the original test data"
      ],
      "metadata": {
        "id": "Ht26OnCAn8ZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = pd.read_csv(\"/content/drive/MyDrive/ADV_DL/Data/Corona_NLP_test.csv\")\n",
        "test_df  = test_df.rename(columns={\"OriginalTweet\":\"text\", \"Sentiment\":\"label\"})\n",
        "\n",
        "label_map = {'Extremely Negative':0,'Negative':1,'Neutral':2,\n",
        "             'Positive':3,'Extremely Positive':4}\n",
        "\n",
        "test_df[\"label\"]  = test_df.label.map(label_map).astype(int)\n",
        "test_df.head()"
      ],
      "metadata": {
        "id": "S5yTcJClneHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading best model using checkpoints"
      ],
      "metadata": {
        "id": "1Url8Laionw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"microsoft/deberta-v3-base\",\n",
        "    num_labels=5\n",
        ")\n",
        "\n",
        "state_dict = torch.load(\"/content/drive/MyDrive/Model_Checkpoints/full_ft_deberta_v3_base/best_model_trial_0.pt\", map_location=device)\n",
        "model.load_state_dict(state_dict)\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "HkwVyyqvo95y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run inference"
      ],
      "metadata": {
        "id": "nlS0kpJ5qE0j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts = test_df[\"text\"].tolist()\n",
        "true_labels = test_df[\"label\"].tolist()\n",
        "\n",
        "batch_size = 32\n",
        "all_preds = []\n",
        "\n",
        "use_amp = (device == \"cuda\")\n",
        "autocast_dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch_texts = texts[i : i + batch_size]\n",
        "        enc = tokenizer(\n",
        "            batch_texts,\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "            max_length=64,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        enc = {k: v.to(device) for k, v in enc.items()}\n",
        "\n",
        "        if use_amp:\n",
        "            with torch.autocast(device_type=\"cuda\", dtype=autocast_dtype):\n",
        "                logits = model(**enc).logits\n",
        "        else:\n",
        "            logits = model(**enc).logits\n",
        "\n",
        "        batch_preds = torch.argmax(logits, dim=-1).cpu().numpy()\n",
        "        all_preds.append(batch_preds)\n",
        "\n",
        "preds = np.concatenate(all_preds, axis=0)\n",
        "\n",
        "# --- Metrics ---\n",
        "acc = accuracy_score(true_labels, preds)\n",
        "f1  = f1_score(true_labels, preds, average=\"weighted\")\n",
        "\n",
        "print(f\"Accuracy: {acc:.4f}\")\n",
        "print(f\"Weighted F1: {f1:.4f}\")"
      ],
      "metadata": {
        "id": "Gn7qMYNWqGNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. HF Fine-Tuned DeBERTa-v3-base"
      ],
      "metadata": {
        "id": "OOc1MlLqnhBJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the original test data"
      ],
      "metadata": {
        "id": "_KQGp1ryn_PP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = pd.read_csv(\"/content/drive/MyDrive/ADV_DL/Data/Corona_NLP_test.csv\")\n",
        "test_df  = test_df.rename(columns={\"OriginalTweet\":\"text\", \"Sentiment\":\"label\"})\n",
        "\n",
        "label_map = {'Extremely Negative':0,'Negative':1,'Neutral':2,\n",
        "             'Positive':3,'Extremely Positive':4}\n",
        "\n",
        "test_df[\"label\"]  = test_df.label.map(label_map).astype(int)\n",
        "test_df.head()"
      ],
      "metadata": {
        "id": "eSMln7Cknkg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the best model"
      ],
      "metadata": {
        "id": "hZKTI522tqRD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"microsoft/deberta-v3-base\",\n",
        "    num_labels=5\n",
        ")\n",
        "\n",
        "# Load safetensors weights\n",
        "state_dict = load_file(\"/content/drive/MyDrive/Model_Checkpoints/hf_ft_deberta_v3_base/model.safetensors\")\n",
        "model.load_state_dict(state_dict)\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "Vp48nL7Qtu8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running Inference"
      ],
      "metadata": {
        "id": "lHxUB4OuuShc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts = test_df[\"text\"].tolist()\n",
        "true_labels = test_df[\"label\"].tolist()\n",
        "\n",
        "batch_size = 32\n",
        "all_preds = []\n",
        "\n",
        "use_amp = (device == \"cuda\")\n",
        "autocast_dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch_texts = texts[i : i + batch_size]\n",
        "        enc = tokenizer(\n",
        "            batch_texts,\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "            max_length=128,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        enc = {k: v.to(device) for k, v in enc.items()}\n",
        "\n",
        "        if use_amp:\n",
        "            with torch.autocast(device_type=\"cuda\", dtype=autocast_dtype):\n",
        "                logits = model(**enc).logits\n",
        "        else:\n",
        "            logits = model(**enc).logits\n",
        "\n",
        "        batch_preds = torch.argmax(logits, dim=-1).cpu().numpy()\n",
        "        all_preds.append(batch_preds)\n",
        "\n",
        "preds = np.concatenate(all_preds, axis=0)\n",
        "\n",
        "# --- Metrics ---\n",
        "acc = accuracy_score(true_labels, preds)\n",
        "f1  = f1_score(true_labels, preds, average=\"weighted\")\n",
        "\n",
        "print(f\"Accuracy: {acc:.4f}\")\n",
        "print(f\"Weighted F1: {f1:.4f}\")"
      ],
      "metadata": {
        "id": "0QQhRhcUuUGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Full-Code Fine-Tuned Twitter-RoBERTa"
      ],
      "metadata": {
        "id": "eFeOjNECnn0G"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cxxSQqqPnn0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. HF Fine-Tuned Twitter-RoBERTa"
      ],
      "metadata": {
        "id": "JQzxRMzInn0H"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5IsOmEUSnn0I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}