{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxMYwoZiHdG9"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "O4gn52jUHZIY"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers accelerate torch pandas scikit-learn tqdm\n",
        "import logging\n",
        "import os, re, json, math, random\n",
        "os.environ[\"TRANSFORMERS_VERBOSITY\"] = \"error\"\n",
        "from transformers.utils import logging as hf_logging\n",
        "hf_logging.set_verbosity_error()                 # transformers’ own logger\n",
        "logging.getLogger(\"transformers\").setLevel(logging.ERROR)  # stdlib logging fallback\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, StoppingCriteria, StoppingCriteriaList\n",
        "from tqdm.auto import tqdm\n",
        "from collections import defaultdict, Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJYZMUO8HnOq"
      },
      "source": [
        "# CONFIG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZzCfh30HqpO"
      },
      "outputs": [],
      "source": [
        "# -----------------------\n",
        "# CONFIG\n",
        "# -----------------------\n",
        "MODEL_ID = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "HF_TOKEN = \"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\"\n",
        "\n",
        "from huggingface_hub import login\n",
        "login(HF_TOKEN)\n",
        "\n",
        "# If you already have your exact validation split in a DataFrame named `eval_df`, set this to True\n",
        "USE_EXISTING_EVAL_DF = False\n",
        "\n",
        "# If not using existing eval_df, point to your raw dataset CSV with columns: OriginalTweet, Sentiment\n",
        "TRAIN_CSV = \"Data/Corona_NLP_train.csv\"\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "# Inference params (deterministic)\n",
        "MAX_NEW_TOKENS = 512\n",
        "TOP_P = 1.0\n",
        "BATCH_SIZE = 8  # tweets per batch for generation (tweak for your GPU)\n",
        "K_PER_CLASS = 2   # few-shot examples per class in each batch\n",
        "MAX_DEMO_LEN  = 200   # truncate long demos for brevity\n",
        "MODEL_ID_R1   = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\n",
        "OUT_CSV_R1    = \"Data/deepseek_zero_shot.csv\"\n",
        "OUT_CSV_R1_FS  = \"Data/deepseek_few_shot.csv\"\n",
        "TEMP_SC       = 0.6      # sampling temperature for self-consistency\n",
        "TOP_P_SC      = 0.95\n",
        "\n",
        "CONF_THRESHOLD = 70        # adaptive SC: if greedy confidence >= 70, accept (B)\n",
        "SC_SAMPLES     = 3         # self-consistency: number of samples per tweet\n",
        "\n",
        "# Output\n",
        "OUT_CSV_A = \"Data/llama_templateA_predictions.csv\"\n",
        "OUT_CSV_B = \"Data/llama_templateB_predictions.csv\"\n",
        "\n",
        "# Canonical labels and numeric mapping (for metrics only)\n",
        "LABELS = [\"Extremely Negative\", \"Negative\", \"Neutral\", \"Positive\", \"Extremely Positive\"]\n",
        "LABEL2ID = {lab: i for i, lab in enumerate(LABELS)}\n",
        "LOWER2CANON = {lab.lower(): lab for lab in LABELS}\n",
        "\n",
        "train_df = pd.read_csv(TRAIN_CSV, encoding=\"latin-1\")\n",
        "\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=RANDOM_STATE, stratify=train_df['Sentiment'])\n",
        "val_df = val_df.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDsYkCx4JHOq"
      },
      "source": [
        "# Llama-3.1-8b-Instruct - Zero-Shot - Steup for running the LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "goKQhZyKJIk-"
      },
      "outputs": [],
      "source": [
        "SYSTEM_PROMPT = (\n",
        "    \"You are a careful annotator for COVID-19 tweet sentiment. \"\n",
        "    \"Follow the instructions exactly and output valid JSON only.\"\n",
        ")\n",
        "\n",
        "USER_TEMPLATE = \"\"\"Classify the sentiment of the tweet below into exactly one of:\n",
        "[\"Extremely Negative\",\"Negative\",\"Neutral\",\"Positive\",\"Extremely Positive\"].\n",
        "\n",
        "Guidelines:\n",
        "- Consider emojis, hashtags, intensifiers (e.g., “soooo”, ALL CAPS), punctuation, and negation.\n",
        "- Sarcasm or jokes: infer the implied attitude where possible.\n",
        "- If the tweet is ambiguous or mixed, choose \"Neutral\".\n",
        "- OUTPUT FORMAT (JSON only):\n",
        "{{\"label\": \"<one of the five labels>\", \"confidence\": <0-100 integer>}}\n",
        "\n",
        "Tweet:\n",
        "{tweet}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-Ba9iqdJPYf"
      },
      "source": [
        "## Load Model + Tokenizer + Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMdtZFRJJQcC"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "dtype  = torch.bfloat16 if device == \"cuda\" else torch.float32\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
        "# Avoid padding warnings for Llama\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    torch_dtype=dtype,\n",
        "    device_map=\"auto\" if device == \"cuda\" else None,\n",
        ")\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=model.dtype,\n",
        "    device_map=\"auto\" if device == \"cuda\" else None,\n",
        "    return_full_text=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqv-SIuDKDq1"
      },
      "source": [
        "## Helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fgmfgJdKEXz"
      },
      "outputs": [],
      "source": [
        "JSON_RE = re.compile(r\"\\{.*\\}\", re.DOTALL)\n",
        "\n",
        "def build_prompts(tweets):\n",
        "    prompts = []\n",
        "    for tw in tweets:\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "            {\"role\": \"user\",   \"content\": USER_TEMPLATE.format(tweet=tw)},\n",
        "        ]\n",
        "        prompts.append(tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))\n",
        "    return prompts\n",
        "\n",
        "def parse_json(text):\n",
        "    if not text:\n",
        "        return None, None\n",
        "    try:\n",
        "        obj = json.loads(text)\n",
        "    except Exception:\n",
        "        m = JSON_RE.search(text)\n",
        "        if not m:\n",
        "            return None, None\n",
        "        try:\n",
        "            obj = json.loads(m.group(0))\n",
        "        except Exception:\n",
        "            return None, None\n",
        "    return obj.get(\"label\"), obj.get(\"confidence\")\n",
        "\n",
        "def normalize_pred_label(label_raw):\n",
        "    if not label_raw:\n",
        "        return None\n",
        "    return LOWER2CANON.get(str(label_raw).strip().lower(), None)\n",
        "\n",
        "def clamp_confidence(c):\n",
        "    try:\n",
        "        x = int(c)\n",
        "        return max(0, min(100, x))\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def generate_batch(tweets):\n",
        "    prompts = build_prompts(tweets)\n",
        "    outs = pipe(\n",
        "        prompts,\n",
        "        max_new_tokens=MAX_NEW_TOKENS,\n",
        "        do_sample=False,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    # pipeline returns list of [ { \"generated_text\": ... } ]\n",
        "    return [o[0][\"generated_text\"] for o in outs]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69GpCshBKxjO"
      },
      "source": [
        "## Inference on Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCpwG1qUKzFI"
      },
      "outputs": [],
      "source": [
        "rows = []\n",
        "num_batches = math.ceil(len(val_df) / BATCH_SIZE)\n",
        "\n",
        "for start in tqdm(range(0, len(val_df), BATCH_SIZE),\n",
        "                  total=num_batches,\n",
        "                  desc=\"Evaluating Llama-3.1 on validation\",\n",
        "                  unit=\"batch\"):\n",
        "    batch = val_df.iloc[start:start+BATCH_SIZE]\n",
        "    gens = generate_batch(batch[\"OriginalTweet\"].tolist())\n",
        "    for i, gen in enumerate(gens):\n",
        "        gold = batch[\"Sentiment\"].iloc[i]\n",
        "        lbl_raw, conf_raw = parse_json(gen)\n",
        "        pred = normalize_pred_label(lbl_raw)\n",
        "        conf = clamp_confidence(conf_raw)\n",
        "        rows.append({\n",
        "            \"OriginalTweet\": batch[\"OriginalTweet\"].iloc[i],\n",
        "            \"Sentiment\": gold,\n",
        "            \"Prediction\": pred,\n",
        "            \"Confidence\": conf,\n",
        "        })\n",
        "\n",
        "pred_df = pd.DataFrame(rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDMxt7IsK__9"
      },
      "outputs": [],
      "source": [
        "valid = pred_df.dropna(subset=[\"Prediction\"]).copy()\n",
        "y_true = valid[\"Sentiment\"].map(LABEL2ID).astype(int).to_numpy()\n",
        "y_pred = valid[\"Prediction\"].map(LABEL2ID).astype(int).to_numpy()\n",
        "\n",
        "acc  = accuracy_score(y_true, y_pred)\n",
        "f1_w = f1_score(y_true, y_pred, average=\"weighted\")\n",
        "f1_m = f1_score(y_true, y_pred, average=\"macro\")\n",
        "\n",
        "print(f\"Validation size: {len(val_df)}\")\n",
        "print(f\"Evaluated (valid predictions): {len(valid)}\")\n",
        "print(f\"Accuracy:   {acc:.4f}\")\n",
        "print(f\"F1 (weighted): {f1_w:.4f}\")\n",
        "print(f\"F1 (macro):    {f1_m:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xplADQOLFGy"
      },
      "outputs": [],
      "source": [
        "pred_df.to_csv(OUT_CSV_A, index=False)\n",
        "print(f\"Saved: {OUT_CSV_A}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rA_Qjiew0bH4"
      },
      "source": [
        "# Llama-3.1-8b-Instruct - Zero-Shot - Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XuIi0q2o0eMr",
        "outputId": "c8a66de5-f9b2-4dde-88dd-fda5ab64f4f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation size: 8232\n",
            "Evaluated (valid predictions): 8231\n",
            "Accuracy:   0.3079\n",
            "F1 (weighted): 0.2765\n",
            "F1 (macro):    0.2597\n"
          ]
        }
      ],
      "source": [
        "pred_df = pd.read_csv(OUT_CSV_A)\n",
        "valid = pred_df.dropna(subset=[\"Prediction\"]).copy()\n",
        "y_true = valid[\"Sentiment\"].map(LABEL2ID).astype(int).to_numpy()\n",
        "y_pred = valid[\"Prediction\"].map(LABEL2ID).astype(int).to_numpy()\n",
        "\n",
        "acc  = accuracy_score(y_true, y_pred)\n",
        "f1_w = f1_score(y_true, y_pred, average=\"weighted\")\n",
        "f1_m = f1_score(y_true, y_pred, average=\"macro\")\n",
        "\n",
        "print(f\"Validation size: {len(val_df)}\")\n",
        "print(f\"Evaluated (valid predictions): {len(valid)}\")\n",
        "print(f\"Accuracy:   {acc:.4f}\")\n",
        "print(f\"F1 (weighted): {f1_w:.4f}\")\n",
        "print(f\"F1 (macro):    {f1_m:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vi_SqldP1E7c"
      },
      "source": [
        "# Llama-3.1-8b-Instruct - Few-Shot - Steup for running the LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VS1N_LWx1Don"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "dtype  = torch.bfloat16 if device == \"cuda\" else torch.float32\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
        "tokenizer.padding_side = \"left\"                  # silence padding warning for decoder-only models\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    torch_dtype=dtype,\n",
        "    device_map=\"auto\" if device == \"cuda\" else None,\n",
        ")\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=model.dtype,\n",
        "    device_map=\"auto\" if device == \"cuda\" else None,\n",
        "    return_full_text=False,   # only the generated continuation\n",
        ")\n",
        "\n",
        "# ----------------------------\n",
        "# Ensure canonical labels in splits (safe even if already canonical)\n",
        "# (Uses your existing train_df, val_df created in CONFIG section)\n",
        "# ----------------------------\n",
        "train_df = train_df.copy()\n",
        "val_df   = val_df.copy()\n",
        "train_df[\"Sentiment\"] = train_df[\"Sentiment\"].astype(str).str.strip().str.lower().map(LOWER2CANON)\n",
        "val_df[\"Sentiment\"]   = val_df[\"Sentiment\"].astype(str).str.strip().str.lower().map(LOWER2CANON)\n",
        "train_df = train_df.dropna(subset=[\"OriginalTweet\",\"Sentiment\"]).reset_index(drop=True)\n",
        "val_df   = val_df.dropna(subset=[\"OriginalTweet\",\"Sentiment\"]).reset_index(drop=True)\n",
        "\n",
        "# ----------------------------\n",
        "# Few-shot support bank FROM TRAIN ONLY (no leakage)\n",
        "# ----------------------------\n",
        "RNG = random.Random(RANDOM_STATE)\n",
        "\n",
        "support_bank = defaultdict(list)\n",
        "for _, row in train_df.iterrows():\n",
        "    tw = str(row[\"OriginalTweet\"]).strip()\n",
        "    if not tw:\n",
        "        continue\n",
        "    if len(tw) > 200:                     # optional: keep examples concise\n",
        "        tw = tw[:197] + \"...\"\n",
        "    support_bank[row[\"Sentiment\"]].append(tw)\n",
        "\n",
        "# Optional: cap very large classes for faster sampling\n",
        "for lab in LABELS:\n",
        "    pool = support_bank.get(lab, [])\n",
        "    if len(pool) > 5000:\n",
        "        support_bank[lab] = RNG.sample(pool, 5000)\n",
        "\n",
        "# ----------------------------\n",
        "# Template B (paper-inspired) prompt pieces\n",
        "# ----------------------------\n",
        "SYSTEM_PROMPT = (\n",
        "    \"You are a careful annotator for COVID-19 tweet sentiment. \"\n",
        "    \"Follow the instructions and output the JSON exactly as requested.\"\n",
        ")\n",
        "\n",
        "SHORT_TAG = {\n",
        "    \"Extremely Negative\":\"ExNeg\",\n",
        "    \"Negative\":\"Neg\",\n",
        "    \"Neutral\":\"Neu\",\n",
        "    \"Positive\":\"Pos\",\n",
        "    \"Extremely Positive\":\"ExPos\",\n",
        "}\n",
        "\n",
        "# Double braces in JSON so .format() does not try to substitute them\n",
        "USER_TEMPLATE_B = \"\"\"{few_shots}\n",
        "Now classify the tweet below into one label from:\n",
        "[\"Extremely Negative\",\"Negative\",\"Neutral\",\"Positive\",\"Extremely Positive\"].\n",
        "\n",
        "Return JSON only: {{\"label\": \"<one of the five labels>\", \"confidence\": <0-100 integer>}}\n",
        "\n",
        "Tweet:\n",
        "{tweet}\n",
        "\"\"\"\n",
        "\n",
        "def build_few_shot_block(k_per_class=2, rng=None):\n",
        "    rng = rng or RNG\n",
        "    lines = [\"Here are labeled examples:\"]\n",
        "    for lab in LABELS:\n",
        "        pool = support_bank.get(lab, [])\n",
        "        if not pool:\n",
        "            continue\n",
        "        k = min(k_per_class, len(pool))\n",
        "        for s in rng.sample(pool, k):\n",
        "            lines.append(f'[{SHORT_TAG[lab]}] \"{s}\" → \"{lab}\"')\n",
        "    lines.append(\"\")  # blank line\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "def build_prompts_few_shot(tweets, k_per_class=2, rng=None):\n",
        "    few_shots = build_few_shot_block(k_per_class=k_per_class, rng=rng)\n",
        "    prompts = []\n",
        "    for tw in tweets:\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "            {\"role\": \"user\",   \"content\": USER_TEMPLATE_B.format(few_shots=few_shots, tweet=tw)},\n",
        "        ]\n",
        "        prompts.append(tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))\n",
        "    return prompts\n",
        "\n",
        "# ----------------------------\n",
        "# Parsing & post-processing helpers\n",
        "# ----------------------------\n",
        "JSON_RE = re.compile(r\"\\{.*\\}\", re.DOTALL)\n",
        "\n",
        "def parse_json(text):\n",
        "    if not text:\n",
        "        return None, None\n",
        "    try:\n",
        "        obj = json.loads(text)\n",
        "    except Exception:\n",
        "        m = JSON_RE.search(text)\n",
        "        if not m:\n",
        "            return None, None\n",
        "        try:\n",
        "            obj = json.loads(m.group(0))\n",
        "        except Exception:\n",
        "            return None, None\n",
        "    return obj.get(\"label\"), obj.get(\"confidence\")\n",
        "\n",
        "def normalize_pred_label(label_raw):\n",
        "    if not label_raw:\n",
        "        return None\n",
        "    return LOWER2CANON.get(str(label_raw).strip().lower(), None)\n",
        "\n",
        "def clamp_confidence(c):\n",
        "    try:\n",
        "        x = int(c)\n",
        "        return max(0, min(100, x))\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# ----------------------------\n",
        "# Inference loop (few-shot per BATCH)\n",
        "# ----------------------------\n",
        "rows = []\n",
        "num_batches = math.ceil(len(val_df) / BATCH_SIZE)\n",
        "\n",
        "for start in tqdm(range(0, len(val_df), BATCH_SIZE),\n",
        "                  total=num_batches,\n",
        "                  desc=\"Evaluating Llama-3.1 (Template B) on validation\",\n",
        "                  unit=\"batch\"):\n",
        "    batch = val_df.iloc[start:start+BATCH_SIZE]\n",
        "    prompts = build_prompts_few_shot(\n",
        "        batch[\"OriginalTweet\"].tolist(),\n",
        "        k_per_class=K_PER_CLASS,\n",
        "        rng=RNG,  # keep fixed for reproducibility; change per batch if you want diversity\n",
        "    )\n",
        "    outs = pipe(\n",
        "        prompts,\n",
        "        max_new_tokens=MAX_NEW_TOKENS,\n",
        "        do_sample=False,                     # deterministic greedy decoding\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    gens = [o[0][\"generated_text\"] for o in outs]\n",
        "\n",
        "    for i, gen in enumerate(gens):\n",
        "        gold = batch[\"Sentiment\"].iloc[i]\n",
        "        lbl_raw, conf_raw = parse_json(gen)\n",
        "        pred = normalize_pred_label(lbl_raw)\n",
        "        conf = clamp_confidence(conf_raw)\n",
        "        rows.append({\n",
        "            \"OriginalTweet\": batch[\"OriginalTweet\"].iloc[i],\n",
        "            \"Sentiment\": gold,\n",
        "            \"Prediction\": pred,\n",
        "            \"Confidence\": conf,\n",
        "        })\n",
        "\n",
        "pred_df = pd.DataFrame(rows)\n",
        "\n",
        "# ----------------------------\n",
        "# Metrics (numeric mapping for calc only)\n",
        "# ----------------------------\n",
        "valid = pred_df.dropna(subset=[\"Prediction\"]).copy()\n",
        "y_true = valid[\"Sentiment\"].map(LABEL2ID).astype(int).to_numpy()\n",
        "y_pred = valid[\"Prediction\"].map(LABEL2ID).astype(int).to_numpy()\n",
        "\n",
        "acc  = accuracy_score(y_true, y_pred)\n",
        "f1_w = f1_score(y_true, y_pred, average=\"weighted\")\n",
        "f1_m = f1_score(y_true, y_pred, average=\"macro\")\n",
        "\n",
        "print(f\"Validation size:            {len(val_df)}\")\n",
        "print(f\"Evaluated (valid preds):    {len(valid)}\")\n",
        "print(f\"Accuracy:                   {acc:.4f}\")\n",
        "print(f\"F1 (weighted):              {f1_w:.4f}\")\n",
        "print(f\"F1 (macro):                 {f1_m:.4f}\")\n",
        "\n",
        "# ----------------------------\n",
        "# Save CSV\n",
        "# ----------------------------\n",
        "pred_df.to_csv(OUT_CSV_B, index=False)\n",
        "print(f\"Saved: {OUT_CSV_B}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IsXOwV0Z5aF"
      },
      "source": [
        "# Llama-3.1-8b-Instruct - Few-Shot - Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0nJSvl6aBxE",
        "outputId": "4ecc38a2-9de7-4265-9f83-fe9ef972fe57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation size: 8232\n",
            "Evaluated (valid predictions): 8232\n",
            "Accuracy:   0.3173\n",
            "F1 (weighted): 0.3019\n",
            "F1 (macro):    0.2984\n"
          ]
        }
      ],
      "source": [
        "pred_df = pd.read_csv(OUT_CSV_B)\n",
        "valid = pred_df.dropna(subset=[\"Prediction\"]).copy()\n",
        "y_true = valid[\"Sentiment\"].map(LABEL2ID).astype(int).to_numpy()\n",
        "y_pred = valid[\"Prediction\"].map(LABEL2ID).astype(int).to_numpy()\n",
        "\n",
        "acc  = accuracy_score(y_true, y_pred)\n",
        "f1_w = f1_score(y_true, y_pred, average=\"weighted\")\n",
        "f1_m = f1_score(y_true, y_pred, average=\"macro\")\n",
        "\n",
        "print(f\"Validation size: {len(val_df)}\")\n",
        "print(f\"Evaluated (valid predictions): {len(valid)}\")\n",
        "print(f\"Accuracy:   {acc:.4f}\")\n",
        "print(f\"F1 (weighted): {f1_w:.4f}\")\n",
        "print(f\"F1 (macro):    {f1_m:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MFmeWwrX2Zk"
      },
      "source": [
        "# Distilled-Deepseek - Zero-Shot - Setup for running the LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qEWo7_D8X-hF"
      },
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# Model / Tokenizer (direct generate for full control)\n",
        "# ----------------------------\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "dtype  = torch.bfloat16 if device == \"cuda\" else torch.float32\n",
        "\n",
        "tokenizer_r1 = AutoTokenizer.from_pretrained(MODEL_ID_R1, use_fast=True)\n",
        "tokenizer_r1.padding_side = \"left\"  # decoder-only models prefer left-padding for generation\n",
        "if tokenizer_r1.pad_token_id is None:\n",
        "    tokenizer_r1.pad_token_id = tokenizer_r1.eos_token_id\n",
        "\n",
        "model_r1 = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID_R1,\n",
        "    torch_dtype=dtype,\n",
        "    device_map=\"auto\" if device == \"cuda\" else None,\n",
        ")\n",
        "\n",
        "# ----------------------------\n",
        "# Zero-shot reasoning prompt (short rationale)  (C)\n",
        "# Use double braces so .format() only fills {tweet}\n",
        "# ----------------------------\n",
        "USER_TEMPLATE_ZS = \"\"\"You are classifying COVID-19 tweets into exactly one label:\n",
        "[\"Extremely Negative\",\"Negative\",\"Neutral\",\"Positive\",\"Extremely Positive\"].\n",
        "\n",
        "Think out loud briefly (max 100 words). Then, as your final output, print EXACTLY ONE line:\n",
        "FINAL_JSON: {{\"label\": \"<one of the five labels>\",\n",
        "             \"rationale\": \"<max 12 words>\",\n",
        "             \"confidence\": <0-100 integer>}}\n",
        "- Do NOT use code fences.\n",
        "- Do NOT add any text after the JSON line.\n",
        "\n",
        "Tweet:\n",
        "{tweet}\n",
        "\"\"\"\n",
        "\n",
        "# ----------------------------\n",
        "# Early stop: halt immediately after first closing brace '}'  (A)\n",
        "# ----------------------------\n",
        "class StopOnJSONEnd(StoppingCriteria):\n",
        "    def __init__(self, tokenizer, pattern=\"}\"):\n",
        "        self.pattern_ids = tokenizer(pattern, add_special_tokens=False).input_ids\n",
        "\n",
        "    def __call__(self, input_ids, scores, **kwargs):\n",
        "        # Simple suffix check\n",
        "        seq = input_ids[0].tolist() if input_ids.dim() == 2 else input_ids.tolist()\n",
        "        L = len(self.pattern_ids)\n",
        "        if L == 0 or len(seq) < L:\n",
        "            return False\n",
        "        return seq[-L:] == self.pattern_ids\n",
        "\n",
        "stopper = StoppingCriteriaList([StopOnJSONEnd(tokenizer_r1, pattern=\"}\")])\n",
        "\n",
        "# ----------------------------\n",
        "# Helpers\n",
        "# ----------------------------\n",
        "\n",
        "def build_prompts_zero_shot(tweets):\n",
        "    prompts = []\n",
        "    for tw in tweets:\n",
        "        messages = [{\"role\": \"user\", \"content\": USER_TEMPLATE_ZS.format(tweet=tw)}]\n",
        "        prompts.append(\n",
        "            tokenizer_r1.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "        )\n",
        "    return prompts\n",
        "\n",
        "JSON_RE_GREEDY     = re.compile(r\"\\{.*\\}\", re.DOTALL)\n",
        "JSON_RE_NONGREEDY  = re.compile(r\"\\{[^{}]*\\}\")\n",
        "FINAL_JSON_ANCHOR  = re.compile(r\"FINAL_JSON\\s*:\\s*(\\{.*\\})\", re.IGNORECASE | re.DOTALL)\n",
        "\n",
        "def parse_json_obj(text: str):\n",
        "    \"\"\"Prefer JSON after 'FINAL_JSON:', else take the last {...} block; else None.\"\"\"\n",
        "    if not text:\n",
        "        return None\n",
        "    # 1) Prefer explicit final JSON\n",
        "    m = FINAL_JSON_ANCHOR.search(text)\n",
        "    if m:\n",
        "        cand = m.group(1).strip()\n",
        "        try:\n",
        "            return json.loads(cand)\n",
        "        except Exception:\n",
        "            pass\n",
        "    # 2) Try last greedy {...}\n",
        "    all_greedy = JSON_RE_GREEDY.findall(text)\n",
        "    for cand in reversed(all_greedy):\n",
        "        try:\n",
        "            return json.loads(cand)\n",
        "        except Exception:\n",
        "            continue\n",
        "    # 3) Try last simple {...}\n",
        "    candidates = JSON_RE_NONGREEDY.findall(text)\n",
        "    for cand in reversed(candidates):\n",
        "        try:\n",
        "            return json.loads(cand)\n",
        "        except Exception:\n",
        "            continue\n",
        "    return None\n",
        "\n",
        "def normalize_pred_label(raw):\n",
        "    if not raw:\n",
        "        return None\n",
        "    return LOWER2CANON.get(str(raw).strip().lower(), None)\n",
        "\n",
        "def clamp_confidence(c):\n",
        "    try:\n",
        "        return max(0, min(100, int(c)))\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def generate_once(prompts, do_sample=False, temperature=0.0, top_p=1.0):\n",
        "    enc = tokenizer_r1(prompts, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    enc = {k: v.to(model_r1.device) for k, v in enc.items()}\n",
        "    with torch.no_grad():\n",
        "        out = model_r1.generate(\n",
        "            **enc,\n",
        "            max_new_tokens=MAX_NEW_TOKENS,\n",
        "            do_sample=do_sample,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            eos_token_id=tokenizer_r1.eos_token_id,\n",
        "        )\n",
        "    prompt_len = enc[\"input_ids\"].shape[1]\n",
        "    gens = tokenizer_r1.batch_decode(out[:, prompt_len:], skip_special_tokens=True)\n",
        "    return gens\n",
        "\n",
        "def majority_vote(prompts, n_samples=SC_SAMPLES, temperature=TEMP_SC, top_p=TOP_P_SC):\n",
        "    \"\"\"\n",
        "    For each prompt, sample n times and majority-vote on the label.\n",
        "    Returns a list of tuples: (label, confidence, rationale, raw_text).\n",
        "    \"\"\"\n",
        "    repeated = [p for p in prompts for _ in range(n_samples)]\n",
        "    gens = generate_once(repeated, do_sample=True, temperature=temperature, top_p=top_p)\n",
        "\n",
        "    results = []\n",
        "    for i in range(0, len(gens), n_samples):\n",
        "        group = gens[i:i+n_samples]\n",
        "        parsed, labels = [], []\n",
        "        for g in group:\n",
        "            obj = parse_json_obj(g)\n",
        "            if obj is None:\n",
        "                parsed.append((None, None, None, g))\n",
        "                continue\n",
        "            lab = normalize_pred_label(obj.get(\"label\"))\n",
        "            conf = clamp_confidence(obj.get(\"confidence\"))\n",
        "            rat = obj.get(\"rationale\")\n",
        "            parsed.append((lab, conf, rat, g))\n",
        "            labels.append(lab)\n",
        "\n",
        "        counts = Counter([l for l in labels if l is not None])\n",
        "        if counts:\n",
        "            voted_label, _ = counts.most_common(1)[0]\n",
        "            # choose among winners (highest confidence preferred)\n",
        "            cands = [t for t in parsed if t[0] == voted_label]  # (lab, conf, rat, raw)\n",
        "            cands.sort(key=lambda t: (t[1] is not None, t[1]), reverse=True)\n",
        "            results.append(cands[0])\n",
        "        else:\n",
        "            # fallback: first valid sample if any, else keep raw of first gen\n",
        "            fb = next((t for t in parsed if t[0] is not None), (None, None, None, group[0]))\n",
        "            results.append(fb)\n",
        "    return results  # (lab, conf, rat, raw)\n",
        "\n",
        "# ----------------------------\n",
        "# Inference (adaptive SC): greedy first, SC only if needed  (B)\n",
        "# ----------------------------\n",
        "rows = []\n",
        "num_batches = math.ceil(len(val_df) / BATCH_SIZE)\n",
        "\n",
        "for start in tqdm(range(0, len(val_df), BATCH_SIZE),\n",
        "                  total=num_batches,\n",
        "                  desc=\"DeepSeek-R1 (zero-shot, adaptive SC)\",\n",
        "                  unit=\"batch\"):\n",
        "    batch = val_df.iloc[start:start+BATCH_SIZE]\n",
        "    prompts = build_prompts_zero_shot(batch[\"OriginalTweet\"].tolist())\n",
        "\n",
        "    # Pass 1: Greedy (fast, deterministic)\n",
        "    greedy_outs = generate_once(prompts, do_sample=False)\n",
        "    parsed_greedy = []\n",
        "    need_sc_idx = []\n",
        "    raw_used = greedy_outs[:]  # start with greedy raw responses\n",
        "\n",
        "    for i, g in enumerate(greedy_outs):\n",
        "        obj = parse_json_obj(g)\n",
        "        if obj is None:\n",
        "            parsed_greedy.append((None, None, None))\n",
        "            need_sc_idx.append(i)\n",
        "            continue\n",
        "        lab = normalize_pred_label(obj.get(\"label\"))\n",
        "        conf = clamp_confidence(obj.get(\"confidence\"))\n",
        "        rat = obj.get(\"rationale\")\n",
        "        if lab is None or conf is None or conf < CONF_THRESHOLD:\n",
        "            need_sc_idx.append(i)\n",
        "        parsed_greedy.append((lab, conf, rat))\n",
        "\n",
        "    # Pass 2: Self-consistency ONLY on low-confidence/invalid items\n",
        "    if need_sc_idx:\n",
        "        sc_prompts = [prompts[i] for i in need_sc_idx]\n",
        "        sc_results = majority_vote(sc_prompts, n_samples=SC_SAMPLES)  # returns (lab, conf, rat, raw)\n",
        "        for j, idx in enumerate(need_sc_idx):\n",
        "            lab, conf, rat, raw = sc_results[j]\n",
        "            parsed_greedy[idx] = (lab, conf, rat)\n",
        "            raw_used[idx] = raw  # override with chosen SC raw output\n",
        "\n",
        "    # Collect rows (now includes RawOutput)\n",
        "    for i, (lab, conf, rat) in enumerate(parsed_greedy):\n",
        "        rows.append({\n",
        "            \"OriginalTweet\": batch[\"OriginalTweet\"].iloc[i],\n",
        "            \"Sentiment\":     batch[\"Sentiment\"].iloc[i],\n",
        "            \"Prediction\":    lab,\n",
        "            \"Confidence\":    conf,\n",
        "            \"Rationale\":     rat,\n",
        "            \"RawOutput\":     raw_used[i],   # <-- full text kept here\n",
        "        })\n",
        "\n",
        "pred_df = pd.DataFrame(rows)\n",
        "\n",
        "# ----------------------------\n",
        "# Metrics (numeric mapping for calc only)\n",
        "# ----------------------------\n",
        "valid = pred_df.dropna(subset=[\"Prediction\"]).copy()\n",
        "y_true = valid[\"Sentiment\"].map(LABEL2ID).astype(int).to_numpy()\n",
        "y_pred = valid[\"Prediction\"].map(LABEL2ID).astype(int).to_numpy()\n",
        "\n",
        "acc  = accuracy_score(y_true, y_pred)\n",
        "f1_w = f1_score(y_true, y_pred, average=\"weighted\")\n",
        "f1_m = f1_score(y_true, y_pred, average=\"macro\")\n",
        "\n",
        "print(f\"Validation size:            {len(val_df)}\")\n",
        "print(f\"Evaluated (valid preds):    {len(valid)}\")\n",
        "print(f\"Accuracy:                   {acc:.4f}\")\n",
        "print(f\"F1 (weighted):              {f1_w:.4f}\")\n",
        "print(f\"F1 (macro):                 {f1_m:.4f}\")\n",
        "\n",
        "pred_df.to_csv(OUT_CSV_R1, index=False)\n",
        "print(f\"Saved: {OUT_CSV_R1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMJ8ufA5aKgA"
      },
      "source": [
        "# Distilled-Deepseek Zero-Shot - Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAvGRxGFaMez",
        "outputId": "b812eec4-7888-4825-d906-4bef73fff8bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation size: 8232\n",
            "Evaluated (valid predictions): 8232\n",
            "Accuracy:   0.3291\n",
            "F1 (weighted): 0.2830\n",
            "F1 (macro):    0.2507\n"
          ]
        }
      ],
      "source": [
        "pred_df = pd.read_csv(OUT_CSV_R1)\n",
        "valid = pred_df.dropna(subset=[\"Prediction\"]).copy()\n",
        "y_true = valid[\"Sentiment\"].map(LABEL2ID).astype(int).to_numpy()\n",
        "y_pred = valid[\"Prediction\"].map(LABEL2ID).astype(int).to_numpy()\n",
        "\n",
        "acc  = accuracy_score(y_true, y_pred)\n",
        "f1_w = f1_score(y_true, y_pred, average=\"weighted\")\n",
        "f1_m = f1_score(y_true, y_pred, average=\"macro\")\n",
        "\n",
        "print(f\"Validation size: {len(val_df)}\")\n",
        "print(f\"Evaluated (valid predictions): {len(valid)}\")\n",
        "print(f\"Accuracy:   {acc:.4f}\")\n",
        "print(f\"F1 (weighted): {f1_w:.4f}\")\n",
        "print(f\"F1 (macro):    {f1_m:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8SmB6x3Zemj"
      },
      "source": [
        "# Distilled-Deepseek Few-Shot - Steup for running the LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mE266pKtZi3I"
      },
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# Model / Tokenizer\n",
        "# ----------------------------\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "dtype  = torch.bfloat16 if device == \"cuda\" else torch.float32\n",
        "\n",
        "tokenizer_r1 = AutoTokenizer.from_pretrained(MODEL_ID_R1, use_fast=True)\n",
        "tokenizer_r1.padding_side = \"left\"  # decoder-only models prefer left padding\n",
        "if tokenizer_r1.pad_token_id is None:\n",
        "    tokenizer_r1.pad_token_id = tokenizer_r1.eos_token_id\n",
        "\n",
        "model_r1 = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID_R1,\n",
        "    torch_dtype=dtype,\n",
        "    device_map=\"auto\" if device == \"cuda\" else None,\n",
        ")\n",
        "\n",
        "# ----------------------------\n",
        "# Few-shot prompt (double braces around JSON example for .format safety)\n",
        "# ----------------------------\n",
        "SHORT_TAG = {\n",
        "    \"Extremely Negative\":\"ExNeg\",\n",
        "    \"Negative\":\"Neg\",\n",
        "    \"Neutral\":\"Neu\",\n",
        "    \"Positive\":\"Pos\",\n",
        "    \"Extremely Positive\":\"ExPos\",\n",
        "}\n",
        "\n",
        "USER_TEMPLATE_FS = \"\"\"You are classifying COVID-19 tweets into exactly one label:\n",
        "[\"Extremely Negative\",\"Negative\",\"Neutral\",\"Positive\",\"Extremely Positive\"].\n",
        "\n",
        "Here are labeled examples:\n",
        "{few_shots}\n",
        "\n",
        "Now classify the tweet below.\n",
        "\n",
        "Think out loud briefly (max 100 words). Then, as your final output, print EXACTLY ONE line:\n",
        "FINAL_JSON: {{\"label\": \"<one of the five labels>\",\n",
        "             \"rationale\": \"<max 12 words>\",\n",
        "             \"confidence\": <0-100 integer>}}\n",
        "- Do NOT use code fences.\n",
        "- Do NOT add any text after the JSON line.\n",
        "\n",
        "Tweet:\n",
        "{tweet}\n",
        "\"\"\"\n",
        "\n",
        "# ----------------------------\n",
        "# Build few-shot support bank FROM TRAIN ONLY (no leakage)\n",
        "# ----------------------------\n",
        "RNG = random.Random(RANDOM_STATE)\n",
        "\n",
        "train_df = train_df.copy()\n",
        "val_df   = val_df.copy()\n",
        "train_df[\"Sentiment\"] = train_df[\"Sentiment\"].astype(str).str.strip().str.lower().map(LOWER2CANON)\n",
        "val_df[\"Sentiment\"]   = val_df[\"Sentiment\"].astype(str).str.strip().str.lower().map(LOWER2CANON)\n",
        "train_df = train_df.dropna(subset=[\"OriginalTweet\",\"Sentiment\"]).reset_index(drop=True)\n",
        "val_df   = val_df.dropna(subset=[\"OriginalTweet\",\"Sentiment\"]).reset_index(drop=True)\n",
        "\n",
        "support_bank = defaultdict(list)\n",
        "for _, row in train_df.iterrows():\n",
        "    tw = str(row[\"OriginalTweet\"]).strip()\n",
        "    if not tw:\n",
        "        continue\n",
        "    if len(tw) > MAX_DEMO_LEN:\n",
        "        tw = tw[:MAX_DEMO_LEN-3] + \"...\"\n",
        "    support_bank[row[\"Sentiment\"]].append(tw)\n",
        "\n",
        "# (Optional) cap huge classes for faster sampling\n",
        "for lab in LABELS:\n",
        "    pool = support_bank.get(lab, [])\n",
        "    if len(pool) > 5000:\n",
        "        support_bank[lab] = RNG.sample(pool, 5000)\n",
        "\n",
        "def build_few_shot_block(k_per_class):\n",
        "    lines = []\n",
        "    for lab in LABELS:\n",
        "        pool = support_bank.get(lab, [])\n",
        "        if not pool:\n",
        "            continue\n",
        "        k = min(k_per_class, len(pool))\n",
        "        for s in RNG.sample(pool, k):\n",
        "            lines.append(f'[{SHORT_TAG[lab]}] \"{s}\" → \"{lab}\"')\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "def build_prompts_few_shot(tweets, k_per_class):\n",
        "    few_shots = build_few_shot_block(k_per_class)\n",
        "    prompts = []\n",
        "    for tw in tweets:\n",
        "        messages = [{\"role\": \"user\",\n",
        "                     \"content\": USER_TEMPLATE_FS.format(few_shots=few_shots, tweet=tw)}]\n",
        "        prompts.append(\n",
        "            tokenizer_r1.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "        )\n",
        "    return prompts\n",
        "\n",
        "# ----------------------------\n",
        "# JSON parsing (prefer FINAL_JSON anchor; soft repairs)\n",
        "# ----------------------------\n",
        "FINAL_JSON_ANCHOR  = re.compile(r\"FINAL_JSON\\s*:\\s*(\\{.*\\})\", re.IGNORECASE | re.DOTALL)\n",
        "JSON_RE_GREEDY     = re.compile(r\"\\{.*\\}\", re.DOTALL)\n",
        "JSON_RE_NONGREEDY  = re.compile(r\"\\{[^{}]*\\}\")\n",
        "\n",
        "def _try_load(s):\n",
        "    try:\n",
        "        return json.loads(s)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def _soft_repair(cand: str):\n",
        "    t = cand.strip().strip(\"`\")\n",
        "    t = re.sub(r\"\\s*```$\", \"\", t)\n",
        "    t = re.sub(r\",\\s*}\", \"}\", t)  # trailing comma before '}'\n",
        "    if not t.endswith(\"}\"):\n",
        "        t = t + \"}\"\n",
        "    return t\n",
        "\n",
        "def parse_json_obj(text: str):\n",
        "    if not text:\n",
        "        return None\n",
        "    # Prefer JSON after 'FINAL_JSON:'\n",
        "    m = FINAL_JSON_ANCHOR.search(text)\n",
        "    if m:\n",
        "        cand = m.group(1).strip()\n",
        "        obj = _try_load(cand) or _try_load(_soft_repair(cand))\n",
        "        if obj is not None:\n",
        "            return obj\n",
        "    # Else: last {...} (greedy)\n",
        "    cands = JSON_RE_GREEDY.findall(text)\n",
        "    for cand in reversed(cands):\n",
        "        obj = _try_load(cand) or _try_load(_soft_repair(cand))\n",
        "        if obj is not None:\n",
        "            return obj\n",
        "    # Else: last simple {...}\n",
        "    cands = JSON_RE_NONGREEDY.findall(text)\n",
        "    for cand in reversed(cands):\n",
        "        obj = _try_load(cand) or _try_load(_soft_repair(cand))\n",
        "        if obj is not None:\n",
        "            return obj\n",
        "    return None\n",
        "\n",
        "def normalize_pred_label(raw):\n",
        "    if not raw:\n",
        "        return None\n",
        "    return LOWER2CANON.get(str(raw).strip().lower(), None)\n",
        "\n",
        "def clamp_confidence(c):\n",
        "    try:\n",
        "        return max(0, min(100, int(c)))\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# ----------------------------\n",
        "# Generation helpers\n",
        "# ----------------------------\n",
        "def generate_once(prompts, do_sample=False, temperature=0.0, top_p=1.0):\n",
        "    enc = tokenizer_r1(prompts, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    enc = {k: v.to(model_r1.device) for k, v in enc.items()}\n",
        "    with torch.no_grad():\n",
        "        out = model_r1.generate(\n",
        "            **enc,\n",
        "            max_new_tokens=MAX_NEW_TOKENS,\n",
        "            do_sample=do_sample,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            eos_token_id=tokenizer_r1.eos_token_id,\n",
        "        )\n",
        "    prompt_len = enc[\"input_ids\"].shape[1]\n",
        "    gens = tokenizer_r1.batch_decode(out[:, prompt_len:], skip_special_tokens=True)\n",
        "    return gens\n",
        "\n",
        "def majority_vote(prompts, n_samples=SC_SAMPLES, temperature=TEMP_SC, top_p=TOP_P_SC):\n",
        "    \"\"\"\n",
        "    For each prompt, sample n times and majority-vote on the label.\n",
        "    Returns a list of tuples: (label, confidence, rationale, raw_text).\n",
        "    \"\"\"\n",
        "    repeated = [p for p in prompts for _ in range(n_samples)]\n",
        "    gens = generate_once(repeated, do_sample=True, temperature=temperature, top_p=top_p)\n",
        "\n",
        "    results = []\n",
        "    for i in range(0, len(gens), n_samples):\n",
        "        group = gens[i:i+n_samples]\n",
        "        parsed, labels = [], []\n",
        "        for g in group:\n",
        "            obj = parse_json_obj(g)\n",
        "            if obj is None:\n",
        "                parsed.append((None, None, None, g))\n",
        "                continue\n",
        "            lab = normalize_pred_label(obj.get(\"label\"))\n",
        "            conf = clamp_confidence(obj.get(\"confidence\"))\n",
        "            rat = obj.get(\"rationale\")\n",
        "            parsed.append((lab, conf, rat, g))\n",
        "            labels.append(lab)\n",
        "\n",
        "        counts = Counter([l for l in labels if l is not None])\n",
        "        if counts:\n",
        "            voted_label, _ = counts.most_common(1)[0]\n",
        "            # choose among winners (highest confidence preferred)\n",
        "            cands = [t for t in parsed if t[0] == voted_label]  # (lab, conf, rat, raw)\n",
        "            cands.sort(key=lambda t: (t[1] is not None, t[1]), reverse=True)\n",
        "            results.append(cands[0])\n",
        "        else:\n",
        "            # fallback: first valid sample if any, else keep raw of first gen\n",
        "            fb = next((t for t in parsed if t[0] is not None), (None, None, None, group[0]))\n",
        "            results.append(fb)\n",
        "    return results  # (lab, conf, rat, raw)\n",
        "\n",
        "# ----------------------------\n",
        "# Inference (few-shot + adaptive SC) with RawOutput logging\n",
        "# ----------------------------\n",
        "rows = []\n",
        "num_batches = math.ceil(len(val_df) / BATCH_SIZE)\n",
        "\n",
        "for start in tqdm(range(0, len(val_df), BATCH_SIZE),\n",
        "                  total=num_batches,\n",
        "                  desc=\"DeepSeek-R1 (few-shot, adaptive SC)\",\n",
        "                  unit=\"batch\"):\n",
        "    batch = val_df.iloc[start:start+BATCH_SIZE]\n",
        "    prompts = build_prompts_few_shot(\n",
        "        batch[\"OriginalTweet\"].tolist(),\n",
        "        k_per_class=K_PER_CLASS\n",
        "    )\n",
        "\n",
        "    # Pass 1: Greedy\n",
        "    greedy_outs = generate_once(prompts, do_sample=False)\n",
        "    parsed_greedy = []\n",
        "    need_sc_idx = []\n",
        "    raw_used = greedy_outs[:]  # start with greedy raw responses\n",
        "\n",
        "    for i, g in enumerate(greedy_outs):\n",
        "        obj = parse_json_obj(g)\n",
        "        if obj is None:\n",
        "            parsed_greedy.append((None, None, None))\n",
        "            need_sc_idx.append(i)\n",
        "            continue\n",
        "        lab = normalize_pred_label(obj.get(\"label\"))\n",
        "        conf = clamp_confidence(obj.get(\"confidence\"))\n",
        "        rat = obj.get(\"rationale\")\n",
        "        if lab is None or conf is None or conf < CONF_THRESHOLD:\n",
        "            need_sc_idx.append(i)\n",
        "        parsed_greedy.append((lab, conf, rat))\n",
        "\n",
        "    # Pass 2: Self-consistency ONLY where needed\n",
        "    if need_sc_idx:\n",
        "        sc_prompts = [prompts[i] for i in need_sc_idx]\n",
        "        sc_results = majority_vote(sc_prompts, n_samples=SC_SAMPLES)  # (lab, conf, rat, raw)\n",
        "        for j, idx in enumerate(need_sc_idx):\n",
        "            lab, conf, rat, raw = sc_results[j]\n",
        "            parsed_greedy[idx] = (lab, conf, rat)\n",
        "            raw_used[idx] = raw  # override with SC raw\n",
        "\n",
        "    # Collect rows (with RawOutput)\n",
        "    for i, (lab, conf, rat) in enumerate(parsed_greedy):\n",
        "        rows.append({\n",
        "            \"OriginalTweet\": batch[\"OriginalTweet\"].iloc[i],\n",
        "            \"Sentiment\":     batch[\"Sentiment\"].iloc[i],  # gold\n",
        "            \"Prediction\":    lab,\n",
        "            \"Confidence\":    conf,\n",
        "            \"Rationale\":     rat,\n",
        "            \"RawOutput\":     raw_used[i],\n",
        "        })\n",
        "\n",
        "pred_df = pd.DataFrame(rows)\n",
        "\n",
        "# ----------------------------\n",
        "# Metrics\n",
        "# ----------------------------\n",
        "valid = pred_df.dropna(subset=[\"Prediction\"]).copy()\n",
        "y_true = valid[\"Sentiment\"].map(LABEL2ID).astype(int).to_numpy()\n",
        "y_pred = valid[\"Prediction\"].map(LABEL2ID).astype(int).to_numpy()\n",
        "\n",
        "acc  = accuracy_score(y_true, y_pred)\n",
        "f1_w = f1_score(y_true, y_pred, average=\"weighted\")\n",
        "f1_m = f1_score(y_true, y_pred, average=\"macro\")\n",
        "\n",
        "print(f\"Validation size:            {len(val_df)}\")\n",
        "print(f\"Evaluated (valid preds):    {len(valid)}\")\n",
        "print(f\"Accuracy:                   {acc:.4f}\")\n",
        "print(f\"F1 (weighted):              {f1_w:.4f}\")\n",
        "print(f\"F1 (macro):                 {f1_m:.4f}\")\n",
        "\n",
        "# Save\n",
        "pred_df.to_csv(OUT_CSV_R1_FS, index=False)\n",
        "print(f\"Saved: {OUT_CSV_R1_FS}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-i-OeXKaTmO"
      },
      "source": [
        "# Distilled-Deepseek Few-Shot - Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqTT4bt7aWjw",
        "outputId": "e7c618e1-34e8-4be2-94d3-444a53b92f8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation size: 8232\n",
            "Evaluated (valid predictions): 8232\n",
            "Accuracy:   0.3213\n",
            "F1 (weighted): 0.2763\n",
            "F1 (macro):    0.2410\n"
          ]
        }
      ],
      "source": [
        "pred_df = pd.read_csv(OUT_CSV_R1_FS)\n",
        "valid = pred_df.dropna(subset=[\"Prediction\"]).copy()\n",
        "y_true = valid[\"Sentiment\"].map(LABEL2ID).astype(int).to_numpy()\n",
        "y_pred = valid[\"Prediction\"].map(LABEL2ID).astype(int).to_numpy()\n",
        "\n",
        "acc  = accuracy_score(y_true, y_pred)\n",
        "f1_w = f1_score(y_true, y_pred, average=\"weighted\")\n",
        "f1_m = f1_score(y_true, y_pred, average=\"macro\")\n",
        "\n",
        "print(f\"Validation size: {len(val_df)}\")\n",
        "print(f\"Evaluated (valid predictions): {len(valid)}\")\n",
        "print(f\"Accuracy:   {acc:.4f}\")\n",
        "print(f\"F1 (weighted): {f1_w:.4f}\")\n",
        "print(f\"F1 (macro):    {f1_m:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyOBZe/SaCbr16kYVXw93kol",
      "collapsed_sections": [
        "zxMYwoZiHdG9",
        "RJYZMUO8HnOq",
        "dDsYkCx4JHOq",
        "vi_SqldP1E7c",
        "4MFmeWwrX2Zk",
        "S8SmB6x3Zemj"
      ],
      "gpuType": "A100",
      "machine_shape": "hm",
      "mount_file_id": "13Jw9-Q3HW-oGxWjIPUyyHgq9OKy3yVtE",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
